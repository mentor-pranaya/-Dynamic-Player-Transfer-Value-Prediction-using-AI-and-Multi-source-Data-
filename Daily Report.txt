Day-1


Today I successfully completed the data collection phase. I gathered player performance datasets from Futbin.com, FBref.com, and StatsBomb Open Data. These sources provide a good mix of detailed match statistics, historical records, and additional performance indicators 

Day-2

StatsBomb Open Data (GitHub Repository)
	-Downloaded the complete StatsBomb Open Data ZIP file from the official GitHub repository.
	-Extracted and stored datasets locally for further use.
	-Verified the folder structure and availability of key JSON files:
	competitions.json (competitions & seasons metadata)
	matches/ (match-level details)
	lineups/ (player participation and squads)
	events/ (event-level data such as passes, shots, fouls, pressures)
	three-sixty/ (freeze-frame positioning data for events)
Transfermarkt Data
	-Downloaded datasets from Transfermarkt (ZIP file).
	-Stored the dataset locally for later preprocessing and analysis.
	-Verified that files were successfully extracted and accessible.
Day-3


1) Combined Dataset Creation:
- Successfully merged data from Futbin (players data from futbin.csv) and Top5 Players (top5-players24-25.xlsx).
- Identified 'similar players' (players common in both datasets) and placed them at the top of the combined file.
- Remaining players (only in Futbin or only in Top5) were added continuously below.

2) Final Dataset Columns:
- Player Performance & Match Data (from Futbin)
- Market Value & Financial Data (from Top5)
- Sentiment & Popularity (placeholder column, to be filled later)
- Injury & Fitness Data (placeholder column)
- Advanced Analytics (placeholder column)

3) Output:
- Generated a combined CSV file `TransferIQ_Combined_Players.csv` containing structured information for further analysis.
- Ensured that similar players were given first preference in row ordering.

Day-4


Today, I successfully completed data scraping for players' sentiment analysis using both the Twitter API and Reddit. The collected data will now help in performing detailed sentiment evaluation and comparison across platforms.

Day-5


Today, I am working on collecting and analyzing players' injury history data to integrate it with the existing sentiment analysis.

Day-6

Today,I started working on the data cleaning and feature engineering 
And done with data collection part from several sources

Day-7

Done with data cleaing process of data set and sentimental analysis 
and started working on master players list

Day-8

Data cleaning is completed for data set,sentimental analysis,injury history files
started making of master list

Day-9

master list completed and cleaning process also completed,working on sql part.

Day-10

Started feature engineering on master_list_final.csv.
Generated additional features such as:
age_experience
contract_risk
total_days_missed
Handled missing values in numeric and categorical columns.
Preliminary data validation completed for feature consistency.

Day-11
Finalized feature engineering for the master list.
Cleaned numeric and categorical features fully.
Prepared final dataset for modeling: master_list_final_features.csv.
Ensured all placeholders for sentiment, injury, and performance metrics are in place.

Day-12
Implemented Random Forest (RF) regression model to predict market_value_in_eur.
Performed train/test split.
Model trained and evaluated:
MSE and R² score calculated.
Saved model: rf_model.pkl
Generated predictions CSV: rf_predictions.csv
Extracted feature importances and saved to feature_importances.csv.

Day-13
Performed exploratory data analysis (EDA). Created correlation heatmaps to see how features relate to market value, checked for multicollinearity, and visualized data distributions with histograms, scatterplots, and boxplots.

Day-14
Applied dimensionality reduction techniques. Used PCA to reduce redundant numeric features and LDA to help separate player positions based on classification. Compared explained variance and decided on the number of components to keep.

Day-15
Tested additional models including Linear Regression as a baseline and XGBoost for more advanced modeling. Compared their performance against Random Forest using RMSE, MAE, and R² metrics. Documented the results.

Day-16
Built a complete prediction pipeline in Python that handles preprocessing (scaling and encoding), automatic feature selection, and outputs transfer value predictions for new player data.
Day-17
I implemented a univariate LSTM model using TensorFlow/Keras to analyze the historical trend of market values. The model focused only on past transfer values to predict future ones. This provided a baseline for how well time-series information alone could explain transfer value changes.
Day-18
Started implementation of a Multivariate LSTM model.
Loaded cleaned_features.npy and cleaned_target.npy files.
Designed an LSTM network with multiple input features (performance, injury, sentiment, etc.).
Defined hyperparameters such as sequence length = 5 and batch size.
Ran initial training and validated that the model architecture was functioning correctly.
Day-19
Focused on Hyperparameter Tuning for the LSTM.
Experimented with different learning rates, hidden units, and dropout values.
Compared performance across models using validation loss and RMSE.
Identified an optimal configuration for stable training.
Documented all results for reproducibility.
Day-20
Integrated Early Stopping and Model Checkpointing into the training loop.
Prevented overfitting by monitoring validation loss.
Saved the best model automatically during training.
Verified improvements in training stability compared to baseline runs.
Started generating predictions and saving them for analysis.
Day-21
Conducted Model Evaluation and Comparison.
Evaluated multivariate LSTM predictions using RMSE, MAE, and R² metrics.
Compared LSTM performance against Random Forest, Linear Regression, and XGBoost results.
Observed that the LSTM captured time-series trends better than traditional models.
Created visualizations (actual vs predicted curves) for selected players.
Day-22
Finalized Model Development Phase.
Cleaned and organized code for reproducibility.
Exported final trained multivariate LSTM model.
Documented experimental setup, tuning parameters, and evaluation results.
Prepared summary for transitioning into the next stage: deployment and interpretability (feature attribution, SHAP, attention mechanisms).
Day 23: Versioning the Data
Snapped CSV versions for datasets. Logged changes: new players added, features updated, bugs fixed. Version control saves sanity.

Day 24: Digging into Sentiment's Role
Checked how sentiment affects value—heatmaps showed positive buzz boosts it a tad. Cool insight; fans matter.

Day 25: Injury Deep Dive
Correlated total_days_missed with value, plotted by position (forwards miss more games?). Trends per role were eye-opening—time to refine.

Day 26: Picking the Best Features
Used RF importances for top 20 features, dropped the rest. Faster training, clearer insights. Less is more.

Day 27: Ensemble Magic
Averaged RF + XGBoost predictions. Ensemble RMSE beat solos slightly—teamwork makes the dream work.

Day 28: Prepping Time-Series for LSTM
Made sequences with 5-match windows, normalized, split train/val/test. LSTM's ready to learn patterns.

Day 29: Bidirectional LSTM Test
Tried bidirectional vs unidirectional—marginal RMSE win for bi. Small gains add up.

Day 30: Normalizing for LSTM
MinMaxScaler on numerics, one-hot on categories. Shapes match now—no input errors.

Day 31: Adding Attention to LSTM
Slapped on an attention layer to spotlight key timesteps. Visuals showed recent matches dominate—makes sense for market values.

Day 32: SHAP for Explainability
SHAP on RF highlighted performance, contract risk, injuries as big influencers, varying by position. Summary plots for the win—stakeholders love this.

Day 33: Stress-Testing Models
Ran on unseen data; stability was solid, but noted some variance. Errors documented—robustness is key.

Day 34: Final LSTM Train
Retrained multivariate on full data, saved as multivariate_lstm_model.keras. Reproducible across runs—phew.

Day 35: Generating Predictions
Predicted values for the whole master list, saved to predicted_transfer_values.csv. Top 50 viz was fun—Mbappé-level stars topping it.

Day 36: Wrapping It Up with Documentation
Compiled scripts, data, results into a final report: methodology, EDA, features, models, evals, viz. Structured for easy reuse or deployment. What a month—learned tons, from data wrangling to LSTMs. If you're building something similar, hit me up!
