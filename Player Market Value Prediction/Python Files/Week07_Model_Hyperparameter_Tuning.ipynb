{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a71532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m543/543\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "âœ… Ensemble predictions saved to ensemble_predictions.csv\n",
      "âœ… Ensemble MAE: 34.20, RMSE: 34.91\n",
      "âœ… Models saved to ensemble_models.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# --- Load Dataset ---\n",
    "data_path = r'C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# --- Encode Categorical Columns ---\n",
    "work_rate_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "df['attacking_work_rate'] = df['attacking_work_rate'].map(work_rate_map)\n",
    "df['defensive_work_rate'] = df['defensive_work_rate'].map(work_rate_map)\n",
    "\n",
    "# --- Feature Selection ---\n",
    "features = [\n",
    "    'overall_rating', 'potential', 'attacking_work_rate', 'defensive_work_rate',\n",
    "    'crossing', 'finishing', 'heading_accuracy', 'short_passing', 'volleys',\n",
    "    'dribbling', 'curve', 'long_passing', 'ball_control', 'acceleration',\n",
    "    'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power', 'jumping',\n",
    "    'stamina', 'strength', 'long_shots', 'aggression', 'interceptions',\n",
    "    'positioning', 'vision', 'penalties', 'marking', 'standing_tackle',\n",
    "    'sliding_tackle', 'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning',\n",
    "    'gk_reflexes', 'injury_count', 'total_days_out', 'days_per_injury'\n",
    "]\n",
    "target = 'overall_rating'\n",
    "\n",
    "# --- Preprocessing ---\n",
    "df = df.dropna(subset=features + ['player_name', target])\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[features + [target]])\n",
    "\n",
    "# --- Sequence Generation ---\n",
    "n_steps = 3\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        seq_x = data[i:i+n_steps, :-1]\n",
    "        seq_y = data[i+n_steps, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_data, n_steps)\n",
    "player_names = df['player_name'].values[n_steps:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "player_test = player_names[-len(y_test):]\n",
    "\n",
    "# --- LSTM Model ---\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(n_steps, X.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# --- LSTM Predictions ---\n",
    "lstm_preds_scaled = model.predict(X_test).flatten()\n",
    "dummy_test = np.zeros((len(y_test), len(features)+1))\n",
    "dummy_preds = np.zeros((len(lstm_preds_scaled), len(features)+1))\n",
    "dummy_test[:, -1] = y_test\n",
    "dummy_preds[:, -1] = lstm_preds_scaled\n",
    "y_test_original = scaler.inverse_transform(dummy_test)[:, -1]\n",
    "lstm_preds_original = scaler.inverse_transform(dummy_preds)[:, -1]\n",
    "\n",
    "# --- XGBoost Model ---\n",
    "X_flat = X.reshape(X.shape[0], -1)\n",
    "X_train_flat, X_test_flat, _, _ = train_test_split(X_flat, y, test_size=0.2, shuffle=False)\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1)\n",
    "xgb_model.fit(X_train_flat, y_train)\n",
    "xgb_preds = xgb_model.predict(X_test_flat)\n",
    "\n",
    "# --- Ensemble Prediction ---\n",
    "final_preds = (lstm_preds_original + xgb_preds) / 2\n",
    "\n",
    "# --- Save Results ---\n",
    "ensemble_df = pd.DataFrame({\n",
    "    \"Player\": player_test,\n",
    "    \"Actual\": y_test_original,\n",
    "    \"LSTM_Pred\": lstm_preds_original,\n",
    "    \"XGB_Pred\": xgb_preds,\n",
    "    \"Final_Pred\": final_preds\n",
    "})\n",
    "ensemble_df.to_csv(\"ensemble_predictions.csv\", index=False)\n",
    "print(\"âœ… Ensemble predictions saved to ensemble_predictions.csv\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "mae = mean_absolute_error(y_test_original, final_preds)\n",
    "rmse = mean_squared_error(y_test_original, final_preds) ** 0.5\n",
    "print(f\"âœ… Ensemble MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# --- Save Models ---\n",
    "joblib.dump({\n",
    "    \"scaler\": scaler,\n",
    "    \"features\": features,\n",
    "    \"target\": target,\n",
    "    \"lstm_model\": model,\n",
    "    \"xgb_model\": xgb_model\n",
    "}, \"ensemble_models.pkl\")\n",
    "print(\"âœ… Models saved to ensemble_models.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f880ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data prepared for LSTM modeling\n",
      "X_train shape: (69489, 3, 39)\n",
      "X_test shape: (17373, 3, 39)\n",
      "y_train shape: (69489,)\n",
      "y_test shape: (17373,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --- Load Dataset ---\n",
    "df = pd.read_csv(r\"C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv\")\n",
    "\n",
    "# --- Encode Categorical Columns ---\n",
    "work_rate_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "df['attacking_work_rate'] = df['attacking_work_rate'].map(work_rate_map)\n",
    "df['defensive_work_rate'] = df['defensive_work_rate'].map(work_rate_map)\n",
    "\n",
    "# --- Valid Feature Set ---\n",
    "features = [\n",
    "    'overall_rating', 'potential', 'attacking_work_rate', 'defensive_work_rate',\n",
    "    'crossing', 'finishing', 'heading_accuracy', 'short_passing', 'volleys',\n",
    "    'dribbling', 'curve', 'long_passing', 'ball_control', 'acceleration',\n",
    "    'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power', 'jumping',\n",
    "    'stamina', 'strength', 'long_shots', 'aggression', 'interceptions',\n",
    "    'positioning', 'vision', 'penalties', 'marking', 'standing_tackle',\n",
    "    'sliding_tackle', 'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning',\n",
    "    'gk_reflexes', 'injury_count', 'total_days_out', 'days_per_injury'\n",
    "]\n",
    "target = 'overall_rating'\n",
    "\n",
    "# --- Drop NaNs and Shuffle ---\n",
    "df = df.dropna(subset=features + [target])\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# --- Scale Features + Target Together ---\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[features + [target]])\n",
    "\n",
    "# --- Create Sequences ---\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        seq_x = data[i:i+n_steps, :-1]\n",
    "        seq_y = data[i+n_steps, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 3\n",
    "X, y = create_sequences(scaled_data, n_steps)\n",
    "\n",
    "# --- Train/Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# --- Output Shapes ---\n",
    "print(\"âœ… Data prepared for LSTM modeling\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ec9d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from week7_tuning\\lstm_tuning\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - loss: 0.0354 - mae: 0.1319 - val_loss: 0.0143 - val_mae: 0.0946\n",
      "Epoch 2/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0141 - mae: 0.0937 - val_loss: 0.0138 - val_mae: 0.0932\n",
      "Epoch 3/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 23ms/step - loss: 0.0138 - mae: 0.0929 - val_loss: 0.0138 - val_mae: 0.0930\n",
      "Epoch 4/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 13ms/step - loss: 0.0138 - mae: 0.0927 - val_loss: 0.0137 - val_mae: 0.0927\n",
      "Epoch 5/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - loss: 0.0138 - mae: 0.0924 - val_loss: 0.0137 - val_mae: 0.0927\n",
      "Epoch 6/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - loss: 0.0136 - mae: 0.0923 - val_loss: 0.0137 - val_mae: 0.0926\n",
      "Epoch 7/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 13ms/step - loss: 0.0135 - mae: 0.0918 - val_loss: 0.0137 - val_mae: 0.0926\n",
      "Epoch 8/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 13ms/step - loss: 0.0134 - mae: 0.0917 - val_loss: 0.0137 - val_mae: 0.0926\n",
      "Epoch 9/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 13ms/step - loss: 0.0137 - mae: 0.0923 - val_loss: 0.0138 - val_mae: 0.0930\n",
      "Epoch 10/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 13ms/step - loss: 0.0137 - mae: 0.0926 - val_loss: 0.0136 - val_mae: 0.0926\n",
      "Epoch 11/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - loss: 0.0136 - mae: 0.0923 - val_loss: 0.0138 - val_mae: 0.0931\n",
      "Epoch 12/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 26ms/step - loss: 0.0136 - mae: 0.0922 - val_loss: 0.0136 - val_mae: 0.0925\n",
      "Epoch 13/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 26ms/step - loss: 0.0135 - mae: 0.0917 - val_loss: 0.0137 - val_mae: 0.0926\n",
      "Epoch 14/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 12ms/step - loss: 0.0135 - mae: 0.0921 - val_loss: 0.0137 - val_mae: 0.0930\n",
      "Epoch 15/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 4ms/step - loss: 0.0135 - mae: 0.0920 - val_loss: 0.0136 - val_mae: 0.0925\n",
      "Epoch 16/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 29ms/step - loss: 0.0134 - mae: 0.0914 - val_loss: 0.0137 - val_mae: 0.0929\n",
      "Epoch 17/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 24ms/step - loss: 0.0135 - mae: 0.0916 - val_loss: 0.0136 - val_mae: 0.0925\n",
      "Epoch 18/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 24ms/step - loss: 0.0135 - mae: 0.0918 - val_loss: 0.0136 - val_mae: 0.0925\n",
      "Epoch 19/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 15ms/step - loss: 0.0136 - mae: 0.0920 - val_loss: 0.0138 - val_mae: 0.0930\n",
      "Epoch 20/20\n",
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.0135 - mae: 0.0919 - val_loss: 0.0136 - val_mae: 0.0926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best Hyperparameters:\n",
      "{'units': 96, 'dropout': 0.0, 'learning_rate': 0.0001}\n",
      "\n",
      "Tuned LSTM - Validation Loss: 0.0136, MAE: 0.0926\n",
      "âœ… Tuned LSTM model saved as best_lstm_week7.h5\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Data Preparation (from Code 2) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv\")\n",
    "\n",
    "work_rate_map = {'low': 0, 'medium': 1, 'high': 2}\n",
    "df['attacking_work_rate'] = df['attacking_work_rate'].map(work_rate_map)\n",
    "df['defensive_work_rate'] = df['defensive_work_rate'].map(work_rate_map)\n",
    "\n",
    "features = [\n",
    "    'overall_rating', 'potential', 'attacking_work_rate', 'defensive_work_rate',\n",
    "    'crossing', 'finishing', 'heading_accuracy', 'short_passing', 'volleys',\n",
    "    'dribbling', 'curve', 'long_passing', 'ball_control', 'acceleration',\n",
    "    'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power', 'jumping',\n",
    "    'stamina', 'strength', 'long_shots', 'aggression', 'interceptions',\n",
    "    'positioning', 'vision', 'penalties', 'marking', 'standing_tackle',\n",
    "    'sliding_tackle', 'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning',\n",
    "    'gk_reflexes', 'injury_count', 'total_days_out', 'days_per_injury'\n",
    "]\n",
    "target = 'overall_rating'\n",
    "\n",
    "df = df.dropna(subset=features + [target])\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df[features + [target]])\n",
    "\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        seq_x = data[i:i+n_steps, :-1]\n",
    "        seq_y = data[i+n_steps, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 3\n",
    "X, y = create_sequences(scaled_data, n_steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# --- Step 2: KerasTuner LSTM Tuning ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_lstm_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        layers.LSTM(\n",
    "            units=hp.Int(\"units\", min_value=32, max_value=128, step=32),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        layers.Dropout(\n",
    "            rate=hp.Choice(\"dropout\", values=[0.0, 0.2, 0.3])\n",
    "        )\n",
    "    )\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_lstm_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"week7_tuning\",\n",
    "    project_name=\"lstm_tuning\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# --- Step 3: Rebuild Best Model Manually ---\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "best_lstm_model = keras.Sequential()\n",
    "best_lstm_model.add(\n",
    "    layers.LSTM(\n",
    "        units=best_hp.get(\"units\"),\n",
    "        activation=\"relu\",\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    )\n",
    ")\n",
    "best_lstm_model.add(\n",
    "    layers.Dropout(rate=best_hp.get(\"dropout\"))\n",
    ")\n",
    "best_lstm_model.add(layers.Dense(1))\n",
    "best_lstm_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=best_hp.get(\"learning_rate\")),\n",
    "    loss=\"mean_squared_error\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "# --- Step 4: Train Best Model ---\n",
    "best_lstm_model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# --- Step 5: Evaluate and Save ---\n",
    "val_loss, val_mae = best_lstm_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"âœ… Best Hyperparameters:\")\n",
    "print(best_hp.values)\n",
    "print(f\"\\nTuned LSTM - Validation Loss: {val_loss:.4f}, MAE: {val_mae:.4f}\")\n",
    "\n",
    "best_lstm_model.save(\"best_lstm_week7.h5\")\n",
    "print(\"âœ… Tuned LSTM model saved as best_lstm_week7.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7248abfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2172/2172\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
      "\u001b[1m543/543\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "=== FINAL MODEL EVALUATION (Week 7) ===\n",
      "Tuned LSTM   â†’ RMSE: 0.1168, MAE: 0.0926, RÂ²: -0.0042\n",
      "Ensemble     â†’ RMSE: 0.1170, MAE: 0.0927, RÂ²: -0.0066\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# === STEP 3: Use tuned LSTM predictions in Ensemble ===\n",
    "\n",
    "# Get predictions from tuned LSTM\n",
    "lstm_train_preds = best_lstm_model.predict(X_train).flatten()\n",
    "lstm_test_preds  = best_lstm_model.predict(X_test).flatten()\n",
    "\n",
    "# Reshape original features for XGBoost (flatten the time steps)\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped  = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Add tuned LSTM predictions as an extra feature\n",
    "X_train_ensemble = np.hstack((X_train_reshaped, lstm_train_preds.reshape(-1,1)))\n",
    "X_test_ensemble  = np.hstack((X_test_reshaped, lstm_test_preds.reshape(-1,1)))\n",
    "\n",
    "# Train XGBoost model\n",
    "xgboost_model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "xgboost_model.fit(X_train_ensemble, y_train)\n",
    "\n",
    "# === STEP 4: Evaluate Models ===\n",
    "\n",
    "# 1. Tuned LSTM\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_test_preds))\n",
    "lstm_mae  = mean_absolute_error(y_test, lstm_test_preds)\n",
    "lstm_r2   = r2_score(y_test, lstm_test_preds)\n",
    "\n",
    "# 2. Ensemble (LSTM + XGBoost)\n",
    "ensemble_preds = xgboost_model.predict(X_test_ensemble)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_preds))\n",
    "ensemble_mae  = mean_absolute_error(y_test, ensemble_preds)\n",
    "ensemble_r2   = r2_score(y_test, ensemble_preds)\n",
    "\n",
    "print(\"\\n=== FINAL MODEL EVALUATION (Week 7) ===\")\n",
    "print(f\"Tuned LSTM   â†’ RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, RÂ²: {lstm_r2:.4f}\")\n",
    "print(f\"Ensemble     â†’ RMSE: {ensemble_rmse:.4f}, MAE: {ensemble_mae:.4f}, RÂ²: {ensemble_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46e13fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Week 7 model comparison saved as 'week7_model_comparison.csv'\n",
      "                     Model      RMSE       MAE        R2\n",
      "0               Tuned LSTM  0.116829  0.092613 -0.004170\n",
      "1  Ensemble (LSTM+XGBoost)  0.116971  0.092661 -0.006597\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results = {\n",
    "    \"Model\": [\"Tuned LSTM\", \"Ensemble (LSTM+XGBoost)\"],\n",
    "    \"RMSE\": [lstm_rmse, ensemble_rmse],\n",
    "    \"MAE\": [lstm_mae, ensemble_mae],\n",
    "    \"R2\": [lstm_r2, ensemble_r2]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"week7_model_comparison.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Week 7 model comparison saved as 'week7_model_comparison.csv'\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c68b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "1 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1143, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 603, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1065, in _create_dmatrix\n",
      "    return QuantileDMatrix(\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 1573, in __init__\n",
      "    self._init(\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 1632, in _init\n",
      "    it.reraise()\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 569, in reraise\n",
      "    raise exc  # pylint: disable=raising-bad-type\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 550, in _handle_exception\n",
      "    return fn()\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 637, in <lambda>\n",
      "    return self._handle_exception(lambda: self.next(input_data), 0)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\data.py\", line 1402, in next\n",
      "    input_data(**self.kwargs)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 626, in input_data\n",
      "    self.proxy.set_info(\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 954, in set_info\n",
      "    self.set_label(label)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 1092, in set_label\n",
      "    dispatch_meta_backend(self, label, \"label\", \"float\")\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\data.py\", line 1338, in dispatch_meta_backend\n",
      "    _meta_from_numpy(data, name, dtype, handle)\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\data.py\", line 1279, in _meta_from_numpy\n",
      "    _check_call(_LIB.XGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "  File \"c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [22:01:02] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\data\\array_interface.cu:44: Check failed: err == cudaGetLastError() (0 vs. 46) : \n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [-0.01370043         nan -0.01366779 -0.01408219 -0.01385195 -0.0136973\n",
      " -0.01364317 -0.01375867 -0.01347642 -0.01345895]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best XGBoost Parameters: {'subsample': 1.0, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.6}\n",
      "\n",
      "ğŸ”¹ Tuned Ensemble (LSTM+XGBoost) â†’ RMSE: 0.1166, MAE: 0.0923, RÂ²: -0.0006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7, 9],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # try 10 random combinations\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_ensemble, y_train)\n",
    "\n",
    "print(\"âœ… Best XGBoost Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Best model\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate tuned XGBoost ensemble\n",
    "ensemble_preds_tuned = best_xgb_model.predict(X_test_ensemble)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "ensemble_rmse_tuned = np.sqrt(mean_squared_error(y_test, ensemble_preds_tuned))\n",
    "ensemble_mae_tuned  = mean_absolute_error(y_test, ensemble_preds_tuned)\n",
    "ensemble_r2_tuned   = r2_score(y_test, ensemble_preds_tuned)\n",
    "\n",
    "print(f\"\\nğŸ”¹ Tuned Ensemble (LSTM+XGBoost) â†’ RMSE: {ensemble_rmse_tuned:.4f}, MAE: {ensemble_mae_tuned:.4f}, RÂ²: {ensemble_r2_tuned:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d3e2863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Updated Week 7 comparison saved as 'week7_model_comparison.csv'\n",
      "                      Model      RMSE       MAE        R2\n",
      "0                Tuned LSTM  0.116829  0.092613 -0.004170\n",
      "1   Ensemble (LSTM+XGBoost)  0.116971  0.092661 -0.006597\n",
      "2  Tuned Ensemble (XGBoost)  0.116624  0.092347 -0.000638\n"
     ]
    }
   ],
   "source": [
    "# Extend comparison DataFrame\n",
    "results = {\n",
    "    \"Model\": [\"Tuned LSTM\", \"Ensemble (LSTM+XGBoost)\", \"Tuned Ensemble (XGBoost)\"],\n",
    "    \"RMSE\": [lstm_rmse, ensemble_rmse, ensemble_rmse_tuned],\n",
    "    \"MAE\": [lstm_mae, ensemble_mae, ensemble_mae_tuned],\n",
    "    \"R2\": [lstm_r2, ensemble_r2, ensemble_r2_tuned]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save updated comparison\n",
    "results_df.to_csv(\"week7_model_comparison.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Updated Week 7 comparison saved as 'week7_model_comparison.csv'\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045219e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(time_steps, X.shape[2])),\n",
    "    LSTM(32),\n",
    "    Dense(1)  # predict market value\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "history = model.fit(X, y, epochs=50, batch_size=8, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64048412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEEK 7 - MULTIVARIATE & ENCODER-DECODER\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# 1. Clean the data\n",
    "transfer_df['Age'] = pd.to_numeric(transfer_df['Age'], errors='coerce')\n",
    "transfer_df['Market Value'] = pd.to_numeric(transfer_df['Market Value'], errors='coerce')\n",
    "\n",
    "transfer_clean = transfer_df[['Market Value', 'Age']].dropna(how=\"any\")\n",
    "\n",
    "print(\"transfer_clean shape:\", transfer_clean.shape)\n",
    "print(transfer_clean.head())\n",
    "\n",
    "# 2. Scale features\n",
    "scaler = MinMaxScaler()\n",
    "features = transfer_clean.values\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# 3. Create sequences\n",
    "time_steps = 5   # hyperparameter: window size\n",
    "def create_sequences(data, time_steps=5):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i+time_steps])\n",
    "        y.append(data[i+time_steps, 0])  # predict Market Value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(scaled_features, time_steps)\n",
    "X = X.reshape((X.shape[0], X.shape[1], features.shape[1]))\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "\n",
    "# 4. Build Encoder-Decoder LSTM\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(time_steps, features.shape[1])),\n",
    "    LSTM(32),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 5. Train model\n",
    "history = model.fit(X, y, epochs=50, batch_size=8, verbose=1)\n",
    "\n",
    "# 6. Evaluate model\n",
    "loss = model.evaluate(X, y, verbose=0)\n",
    "print(\"Final MSE Loss:\", loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv Infosys)",
   "language": "python",
   "name": "infosys_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
