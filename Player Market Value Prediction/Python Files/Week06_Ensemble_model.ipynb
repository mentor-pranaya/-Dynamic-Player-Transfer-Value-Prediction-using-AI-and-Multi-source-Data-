{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04554959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load your pre-processed dataset\n",
    "df = pd.read_csv(r'C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv')\n",
    "\n",
    "# Define features and target (same as Week 5)\n",
    "features = ['passes_attempted', 'expected_goals', 'goals', 'assists', 'injury_count', 'total_days_out', 'avg_market_value']\n",
    "target = 'avg_market_value'\n",
    "\n",
    "# Create time-series sequences and split data (same as Week 5)\n",
    "n_steps = 3\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(data)-1:\n",
    "            break\n",
    "        seq_x, seq_y = data[i:end_ix, :], data[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale data and create sequences\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[features])\n",
    "X, y = create_sequences(scaled_data, n_steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Load the trained LSTM model (assuming you saved it)\n",
    "# lstm_model = load_model('lstm_model.h5')\n",
    "# Note: Since the model was not saved, you will use the one created in the previous session.\n",
    "# If you restart your kernel, you will need to re-run the Week 5 code to recreate the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from the LSTM model on the training set\n",
    "lstm_train_preds = model.predict(X_train).flatten()\n",
    "\n",
    "# Reshape the original training data for XGBoost (flatten the time steps)\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Add the LSTM predictions as a new column to the training data\n",
    "X_train_ensemble = np.hstack((X_train_reshaped, lstm_train_preds.reshape(-1, 1)))\n",
    "\n",
    "print(\"Original training data shape for XGBoost:\", X_train_reshaped.shape)\n",
    "print(\"Ensemble training data shape with LSTM predictions:\", X_train_ensemble.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1c9cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model has been successfully rebuilt and trained.\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\n",
      "Ensemble training data shape with LSTM predictions: (916, 22)\n",
      "Ensemble test data shape with LSTM predictions: (229, 22)\n",
      "\n",
      "XGBoost model trained successfully.\n",
      "LSTM Model RMSE on Test Set: 0.1092\n",
      "XGBoost Ensemble Model RMSE on Test Set: 0.1200\n",
      "\n",
      "⚠️ The ensemble model's performance was not an improvement over the standalone LSTM model. You may need to tune hyperparameters or try a different stacking approach.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# --- WEEK 5 CODE: DATA PREPARATION AND MODEL TRAINING ---\n",
    "# Load the consolidated dataset\n",
    "# Note: You need to replace the local path with the correct file name if it's in the same directory\n",
    "df = pd.read_csv(r'C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['passes_attempted', 'expected_goals', 'goals', 'assists', 'injury_count', 'total_days_out', 'avg_market_value']\n",
    "target = 'avg_market_value'\n",
    "\n",
    "# Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[features])\n",
    "\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(data) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = data[i:end_ix, :], data[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 3\n",
    "X, y = create_sequences(scaled_data, n_steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Re-build and re-train the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n",
    "print(\"LSTM model has been successfully rebuilt and trained.\")\n",
    "\n",
    "# --- WEEK 6 CODE: ENSEMBLE MODELING ---\n",
    "# Generate predictions from the LSTM model on the training and test sets\n",
    "lstm_train_preds = model.predict(X_train).flatten()\n",
    "lstm_test_preds = model.predict(X_test).flatten()\n",
    "\n",
    "# Reshape the original data for XGBoost (flatten the time steps)\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Add the LSTM predictions as a new column to the training and test data\n",
    "X_train_ensemble = np.hstack((X_train_reshaped, lstm_train_preds.reshape(-1, 1)))\n",
    "X_test_ensemble = np.hstack((X_test_reshaped, lstm_test_preds.reshape(-1, 1)))\n",
    "\n",
    "print(\"\\nEnsemble training data shape with LSTM predictions:\", X_train_ensemble.shape)\n",
    "print(\"Ensemble test data shape with LSTM predictions:\", X_test_ensemble.shape)\n",
    "\n",
    "# Build and train the XGBoost model\n",
    "xgboost_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgboost_model.fit(X_train_ensemble, y_train)\n",
    "print(\"\\nXGBoost model trained successfully.\")\n",
    "\n",
    "# Make final predictions with the XGBoost ensemble model\n",
    "final_preds_scaled = xgboost_model.predict(X_test_ensemble)\n",
    "\n",
    "# Calculate RMSE for the standalone LSTM model (on the test set)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_test_preds))\n",
    "print(f\"LSTM Model RMSE on Test Set: {lstm_rmse:.4f}\")\n",
    "\n",
    "# Calculate RMSE for the final ensemble model\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, final_preds_scaled))\n",
    "print(f\"XGBoost Ensemble Model RMSE on Test Set: {ensemble_rmse:.4f}\")\n",
    "\n",
    "# Compare the results\n",
    "if ensemble_rmse < lstm_rmse:\n",
    "    print(\"\\n✅ The ensemble model performed better than the standalone LSTM model!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ The ensemble model's performance was not an improvement over the standalone LSTM model. You may need to tune hyperparameters or try a different stacking approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bab46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data prepared\n",
      "X_train shape: (916, 3, 7) y_train shape: (916,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\.venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM model trained\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Ensemble Train Shape: (916, 22)\n",
      "Ensemble Test Shape: (229, 22)\n",
      "✅ XGBoost model trained\n",
      "\n",
      "📊 LSTM RMSE on Test Set: 0.1093\n",
      "📊 Ensemble RMSE on Test Set: 0.1146\n",
      "⚠️ Ensemble did not improve, consider tuning hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 📌 Week 6: Ensemble Modeling\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# 1️⃣ Load Data\n",
    "df = pd.read_csv(r\"C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv\")\n",
    "\n",
    "features = ['passes_attempted', 'expected_goals', 'goals', 'assists',\n",
    "            'injury_count', 'total_days_out', 'avg_market_value']\n",
    "target = 'avg_market_value'\n",
    "\n",
    "# 2️⃣ Preprocess Data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[features])\n",
    "\n",
    "def create_sequences(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(data)-1:\n",
    "            break\n",
    "        seq_x, seq_y = data[i:end_ix, :], data[end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_steps = 3\n",
    "X, y = create_sequences(scaled_data, n_steps)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(\"✅ Data prepared\")\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "\n",
    "# 3️⃣ Build & Train LSTM\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(n_steps, X.shape[2])),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "print(\"✅ LSTM model trained\")\n",
    "\n",
    "# 4️⃣ LSTM Predictions\n",
    "lstm_train_preds = model.predict(X_train).flatten()\n",
    "lstm_test_preds = model.predict(X_test).flatten()\n",
    "\n",
    "# 5️⃣ Prepare Ensemble Input for XGBoost\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "X_train_ensemble = np.hstack((X_train_reshaped, lstm_train_preds.reshape(-1,1)))\n",
    "X_test_ensemble = np.hstack((X_test_reshaped, lstm_test_preds.reshape(-1,1)))\n",
    "\n",
    "print(\"Ensemble Train Shape:\", X_train_ensemble.shape)\n",
    "print(\"Ensemble Test Shape:\", X_test_ensemble.shape)\n",
    "\n",
    "# 6️⃣ Train XGBoost\n",
    "xgboost_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "xgboost_model.fit(X_train_ensemble, y_train)\n",
    "print(\"✅ XGBoost model trained\")\n",
    "\n",
    "# 7️⃣ Evaluate Models\n",
    "final_preds_scaled = xgboost_model.predict(X_test_ensemble)\n",
    "\n",
    "lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_test_preds))\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_test, final_preds_scaled))\n",
    "\n",
    "print(f\"\\n📊 LSTM RMSE on Test Set: {lstm_rmse:.4f}\")\n",
    "print(f\"📊 Ensemble RMSE on Test Set: {ensemble_rmse:.4f}\")\n",
    "\n",
    "if ensemble_rmse < lstm_rmse:\n",
    "    print(\"✅ Ensemble improved over LSTM!\")\n",
    "else:\n",
    "    print(\"⚠️ Ensemble did not improve, consider tuning hyperparameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE WEEK 6 MODELS & RESULTS ---\n",
    "\n",
    "# Save rebuilt LSTM model\n",
    "model.save(\"week6_lstm_model.h5\")\n",
    "print(\"✅ Rebuilt LSTM model saved as week6_lstm_model.h5\")\n",
    "\n",
    "# Save trained XGBoost ensemble model\n",
    "import joblib\n",
    "joblib.dump(xgboost_model, \"week6_xgboost_model.pkl\")\n",
    "print(\"✅ XGBoost model saved as week6_xgboost_model.pkl\")\n",
    "\n",
    "# Align player names with y_test\n",
    "df = pd.read_csv(r\"C:\\Users\\M.ANTONY ROJES\\Downloads\\Infosys\\data\\feature_engineered\\player_features_model_all_imputed.csv\")\n",
    "n_steps = 3\n",
    "player_names = df['player'][len(df) - len(y_test):].reset_index(drop=True)\n",
    "# Save predictions and RMSE results with player names\n",
    "results_df = pd.DataFrame({\n",
    "    \"player\": player_names,\n",
    "    \"y_test\": y_test,\n",
    "    \"lstm_preds\": lstm_test_preds,\n",
    "    \"ensemble_preds\": final_preds_scaled\n",
    "})\n",
    "results_df.to_csv(\"week6_predictions.csv\", index=False)\n",
    "print(\"✅ Predictions saved as week6_predictions.csv with player names\")\n",
    "\n",
    "# Save RMSE results\n",
    "with open(\"week6_results.txt\", \"w\") as f:\n",
    "    f.write(f\"LSTM RMSE (Test Set): {lstm_rmse:.4f}\\n\")\n",
    "    f.write(f\"Ensemble RMSE (Test Set): {ensemble_rmse:.4f}\\n\")\n",
    "print(\"✅ RMSE results saved as week6_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4e3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LSTM Evaluation]\n",
      "RMSE: 0.1095\n",
      "MAE : 0.0620\n",
      "R²  : -0.0287\n",
      "\n",
      "[Ensemble (LSTM+XGBoost) Evaluation]\n",
      "RMSE: 0.1118\n",
      "MAE : 0.0603\n",
      "R²  : -0.0728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.11180233035438548), 0.060325400326593094, -0.07277141394520403)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n[{model_name} Evaluation]\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE : {mae:.4f}\")\n",
    "    print(f\"R²  : {r2:.4f}\")\n",
    "    return rmse, mae, r2\n",
    "\n",
    "# Example usage with Week 6 predictions:\n",
    "evaluate_model(y_test, lstm_test_preds, \"LSTM\")\n",
    "evaluate_model(y_test, final_preds_scaled, \"Ensemble (LSTM+XGBoost)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv Infosys)",
   "language": "python",
   "name": "infosys_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
